{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE FUNCTIONS <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def get_access_token(client_id:str, client_sc:str):\n",
    "    import requests\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    }\n",
    "    data = f'grant_type=client_credentials&client_id={client_id}&client_secret={client_sc}'.encode()\n",
    "    response = requests.post('https://accounts.spotify.com/api/token', headers=headers, data=data).json()\n",
    "    access_token = response['access_token']\n",
    "\n",
    "    return access_token\n",
    "\n",
    "def get_response(access_token:str, endpoint:str, params:dict=None):\n",
    "    import requests, json\n",
    "\n",
    "    url = f\"https://api.spotify.com/v1/{endpoint}\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}',\n",
    "    }\n",
    "\n",
    "    if params != None:\n",
    "        response = requests.get(url=url, params=params, headers=headers)\n",
    "    else:\n",
    "        response = requests.get(url=url, headers=headers)\n",
    "    print(response)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            return data\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            raise ValueError(f\"API Server Error - {endpoint} - Invalid JSON content in response: {response.text}\")\n",
    "    else:\n",
    "        raise ValueError(f\"API Server Error - {endpoint} - Non-200 status code received: {response.status_code}\")\n",
    "    \n",
    "\n",
    "def post_response(access_token:str, endpoint:str, data:dict=None):\n",
    "    import requests\n",
    "\n",
    "    url = f\"https://api.spotify.com/v1/{endpoint}\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}',\n",
    "    }\n",
    "\n",
    "    response = requests.post(url=url, headers=headers)\n",
    "    print(response)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"API Server Error - {endpoint} - Non-200 status code received: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFOS <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "from configparser import ConfigParser\n",
    "\n",
    "config = ConfigParser()\n",
    "config.read(\"/home/hooniegit/git/Spotify-DemoProject/recommendation/demo/config.ini\")\n",
    "\n",
    "client_id = config.get(\"spotify\", \"client_id\")\n",
    "client_sc = config.get(\"spotify\", \"client_sc\")\n",
    "user_id = config.get(\"spotify\", \"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START CODE <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from math import ceil\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Build Session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(config.get(\"spark\", \"master\")) \\\n",
    "    .appName(\"pipeline_demo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "### Create Access Token\n",
    "access_token = get_access_token(client_id=client_id, client_sc=client_sc)\n",
    "\n",
    "### Create Playlist Lists\n",
    "endpoint = f\"users/{user_id}/playlists\"\n",
    "params = {\n",
    "    \"limit\": 50,\n",
    "    \"offset\": 0\n",
    "}\n",
    "\n",
    "playlists = get_response(access_token=access_token, endpoint=endpoint, params=params)\n",
    "json_string  = json.dumps(playlists)\n",
    "json_rdd = spark.sparkContext.parallelize([json_string])\n",
    "df_plinfo = spark.read.json(json_rdd, multiLine=True)\n",
    "\n",
    "items = df_plinfo \\\n",
    "    .withColumn(\"items\", explode(\"items\")) \\\n",
    "    .select(\"items.id\") \\\n",
    "    .rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "### Create Playlist Item Lists\n",
    "track_list = [] # <---------- \"Need To Use\"\n",
    "for id in items:\n",
    "    endpoint = f\"playlists/{id}/tracks\"\n",
    "    playlist_spec = get_response(access_token=access_token, endpoint=endpoint)\n",
    "    \n",
    "    json_string  = json.dumps(playlist_spec)\n",
    "    json_rdd = spark.sparkContext.parallelize([json_string])\n",
    "    df_playlist_spec = spark.read.json(json_rdd, multiLine=True)\n",
    "    \n",
    "    ids = df_playlist_spec \\\n",
    "    .withColumn(\"items\", explode(\"items\")) \\\n",
    "    .select(\"items.track.id\") \\\n",
    "    .rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    track_list += ids\n",
    "    \n",
    "    total = df_playlist_spec.select(\"total\").first()[0]\n",
    "    print(total)\n",
    "    left = int(total)-100\n",
    "    cnt = ceil(left/100)\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        offset = 100 + 100 * i\n",
    "        params = {\"offset\":offset}\n",
    "        \n",
    "        playlist_spec = get_response(access_token=access_token, endpoint=endpoint, params=params)\n",
    "        \n",
    "        json_string  = json.dumps(playlist_spec)\n",
    "        json_rdd = spark.sparkContext.parallelize([json_string])\n",
    "        df_playlist_spec = spark.read.json(json_rdd, multiLine=True)\n",
    "        \n",
    "        ids = df_playlist_spec \\\n",
    "        .withColumn(\"items\", explode(\"items\")) \\\n",
    "        .select(\"items.track.id\") \\\n",
    "        .rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "        track_list += ids      \n",
    "\n",
    "cnt = ceil(len(track_list)/50)\n",
    "\n",
    "big_list = []\n",
    "for j in range(cnt):\n",
    "    big_list.append(track_list[j*50:(j+1)*50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "1\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "2\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# Create Dataframe : main_df\n",
    "main_df = None\n",
    "cnt = 0\n",
    "for small_list in big_list:\n",
    "    \n",
    "    print(cnt)\n",
    "    \n",
    "    tracks = \"\"\n",
    "    for id in small_list:\n",
    "        tracks += f\",{id}\"\n",
    "    tracks = tracks[1:]\n",
    "    \n",
    "    endpoint = \"tracks\"\n",
    "    params = {\"ids\":tracks}\n",
    "    track = get_response(access_token=access_token, endpoint=endpoint, params=params)\n",
    "    \n",
    "    json_string  = json.dumps(track)\n",
    "    json_rdd = spark.sparkContext.parallelize([json_string])\n",
    "    df_tracks = spark.read.json(json_rdd, multiLine=True)\n",
    "    \n",
    "    df_tracks = spark.read.json(json_rdd, multiLine=True) \\\n",
    "        .withColumn(\"tracks\", explode(\"tracks\")) \\\n",
    "        .selectExpr(\"tracks.id\",\n",
    "                    \"tracks.popularity\")\n",
    "    \n",
    "    endpoint = \"audio-features\"\n",
    "    params = {\"ids\":tracks}\n",
    "    audio_features = get_response(access_token=access_token, endpoint=endpoint, params=params)\n",
    "    \n",
    "    json_string  = json.dumps(audio_features)\n",
    "    json_rdd = spark.sparkContext.parallelize([json_string])\n",
    "    df_audio_features = spark.read.json(json_rdd, multiLine=True) \\\n",
    "        .withColumn(\"audio_features\", explode(\"audio_features\")) \\\n",
    "        .selectExpr(\"audio_features.id\",\n",
    "                    \"audio_features.key\",\n",
    "                    \"audio_features.mode\",\n",
    "                    \"audio_features.time_signature\",\n",
    "                    \"audio_features.tempo\",\n",
    "                    \"audio_features.acousticness\",\n",
    "                    \"audio_features.danceability\",\n",
    "                    \"audio_features.energy\",\n",
    "                    \"audio_features.instrumentalness\",\n",
    "                    \"audio_features.liveness\",\n",
    "                    \"audio_features.loudness\",\n",
    "                    \"audio_features.speechiness\",\n",
    "                    \"audio_features.valence\")\n",
    "    \n",
    "    result_track_df = df_tracks.join(df_audio_features, \"id\", \"left\")\n",
    "    if cnt == 0:\n",
    "        main_df = result_track_df\n",
    "    else:\n",
    "        main_df = main_df.union(result_track_df)\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 155:===================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---+----+--------------+-------+------------+------------+------+----------------+--------+--------+-----------+-------+\n",
      "|                  id|popularity|key|mode|time_signature|  tempo|acousticness|danceability|energy|instrumentalness|liveness|loudness|speechiness|valence|\n",
      "+--------------------+----------+---+----+--------------+-------+------------+------------+------+----------------+--------+--------+-----------+-------+\n",
      "|000O8nXpAK5QAppKv...|         5|  1|   1|             4|128.018|     5.12E-4|       0.504|  0.93|          0.0107|   0.281|   -2.89|     0.0539|  0.355|\n",
      "|000kSCs9tKtH1VXI3...|        11|  2|   1|             4| 129.85|       0.863|       0.425| 0.266|             0.0|  0.0989|  -6.791|     0.0337|  0.253|\n",
      "|0010mZpCCwlPwoBiB...|        38|  3|   1|             3|124.993|       0.108|       0.527| 0.793|         3.28E-6|   0.144|  -4.823|     0.0352|  0.597|\n",
      "|0026hQeV7FZ0PaZpW...|        20| 11|   1|             4|169.358|        0.33|        0.48| 0.737|             0.0|   0.449|  -7.023|      0.216|  0.201|\n",
      "|003Zj5utQfbYDofOq...|        19|  1|   1|             4| 89.971|      0.0117|       0.706| 0.478|          0.0211|   0.116|  -7.839|     0.0434| 0.0869|\n",
      "|003heort6ZPgg46RM...|         2|  8|   1|             4| 81.486|       0.886|        0.72| 0.231|           0.806|   0.118|  -14.43|     0.0364|  0.742|\n",
      "|004zTdXYH400nJeJH...|         3|  8|   0|             5|168.081|       0.956|       0.254| 0.133|           0.506|  0.0559| -27.071|     0.0831| 0.0702|\n",
      "|006x4IC6Fm1xjYtns...|         0|  6|   0|             3|  63.28|       0.912|       0.166| 0.547|         2.32E-4|   0.933| -14.973|     0.0799|  0.234|\n",
      "|007dnwwA3q2fhQ4AC...|         0| 10|   1|             4|139.869|      0.0181|       0.746|  0.82|             0.0|   0.418|  -1.596|     0.0503|  0.271|\n",
      "|007qEKeXFHJ2r7rMU...|        25|  6|   0|             4|103.515|      0.0743|       0.707| 0.523|             0.0|   0.252|  -5.863|      0.228|  0.605|\n",
      "|00841Yho1fVUDDfA9...|         0| 11|   0|             5|180.478|       0.534|       0.613|0.0981|             0.0|   0.226| -27.133|      0.955|  0.609|\n",
      "|0099y8U9Px3fIJX5L...|         0|  3|   1|             4| 91.476|       0.772|       0.541| 0.462|             0.0|  0.0683|  -6.676|     0.0249|  0.461|\n",
      "|009BdcwF2lB7RZiTS...|        11| 10|   1|             4| 78.336|       0.915|       0.219|   0.2|           0.321|   0.284|  -11.52|     0.0324|  0.192|\n",
      "|009ltBhNaq5XDC4f3...|        10|  9|   0|             3|157.619|       0.931|       0.452| 0.139|             0.0|  0.0851| -15.348|     0.0407|  0.336|\n",
      "|00AYIzevROJfCstNK...|        17|  8|   0|             4| 90.218|       0.105|       0.531| 0.909|             0.0|  0.0858|  -3.525|       0.29|  0.644|\n",
      "|00B1efLzCB1lUNPT8...|         3|  0|   1|             4|171.374|       0.849|       0.485| 0.362|         4.13E-6|  0.0986|  -7.889|     0.0889|  0.284|\n",
      "|00BhvVsOJkBMJVyoj...|        21|  7|   0|             4| 81.911|       0.709|       0.624| 0.654|          3.6E-5|  0.0974|  -4.538|     0.0545|  0.329|\n",
      "|00CNYGlhmuXQcJU81...|         2|  2|   1|             1|124.193|      0.0473|       0.505| 0.753|         0.00156|   0.181|  -9.898|     0.0348|  0.535|\n",
      "|00CYT9MwnEJva7cCy...|         9|  1|   1|             4|105.079|       0.784|       0.867|  0.25|             0.0|   0.132|  -7.028|     0.0358|  0.442|\n",
      "|00CZIZmqtInPEWdxG...|        13|  4|   1|             5|  83.73|       0.958|       0.372| 0.246|             0.0|  0.0741| -10.112|     0.0582|  0.257|\n",
      "+--------------------+----------+---+----+--------------+-------+------------+------------+------+----------------+--------+--------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Load Dataframe : df_dw\n",
    "dw_tracks = spark.read.parquet(\"file:///home/hooniegit/git/Spotify-DemoProject/spark/data/parquet/tracks/main/*\")\n",
    "dw_audioFeatures = spark.read.parquet(\"file:///home/hooniegit/git/Spotify-DemoProject/spark/data/parquet/tracks/audio_features/*\")\n",
    "df_dw = dw_tracks.join(dw_audioFeatures, \"id\", \"inner\")\n",
    "\n",
    "df_dw.show() # << TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2751:============> (10 + 1) / 11][Stage 2752:=============>(16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1581470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(main_df.count())\n",
    "print(df_dw.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1581569"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Union Dataframe : df\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df_dw \\\n",
    "    .filter(~col(\"id\").isin(track_list)) \\\n",
    "    .union(main_df)\n",
    "\n",
    "df.count() # << TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고려해야 할 사항들\n",
    "\"\"\"\n",
    "    0. 스케일링 가능 항목 : acousticness, danceability, energy, instrumentalness, liveness, speechiness, valence\n",
    "    1. loudness 항목 : 절댓값 반환 필요 / 계산식 확인 필요\n",
    "    1. mode 항목 : 0 또는 1만 가짐\n",
    "    2. key, tempo 항목 : 이상치가 큰 영향을 끼칠 수 있음\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2815:==================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---+----+--------------+-------+------------+------------+------+----------------+--------+--------+-----------+-------+\n",
      "|                  id|popularity|key|mode|time_signature|  tempo|acousticness|danceability|energy|instrumentalness|liveness|loudness|speechiness|valence|\n",
      "+--------------------+----------+---+----+--------------+-------+------------+------------+------+----------------+--------+--------+-----------+-------+\n",
      "|000O8nXpAK5QAppKv...|         5|  1|   1|             4|128.018|     5.12E-4|       0.504|  0.93|          0.0107|   0.281|    2.89|     0.0539|  0.355|\n",
      "|000kSCs9tKtH1VXI3...|        11|  2|   1|             4| 129.85|       0.863|       0.425| 0.266|             0.0|  0.0989|   6.791|     0.0337|  0.253|\n",
      "|0010mZpCCwlPwoBiB...|        38|  3|   1|             3|124.993|       0.108|       0.527| 0.793|         3.28E-6|   0.144|   4.823|     0.0352|  0.597|\n",
      "|0026hQeV7FZ0PaZpW...|        20| 11|   1|             4|169.358|        0.33|        0.48| 0.737|             0.0|   0.449|   7.023|      0.216|  0.201|\n",
      "|003Zj5utQfbYDofOq...|        19|  1|   1|             4| 89.971|      0.0117|       0.706| 0.478|          0.0211|   0.116|   7.839|     0.0434| 0.0869|\n",
      "|003heort6ZPgg46RM...|         2|  8|   1|             4| 81.486|       0.886|        0.72| 0.231|           0.806|   0.118|   14.43|     0.0364|  0.742|\n",
      "|004zTdXYH400nJeJH...|         3|  8|   0|             5|168.081|       0.956|       0.254| 0.133|           0.506|  0.0559|  27.071|     0.0831| 0.0702|\n",
      "|006x4IC6Fm1xjYtns...|         0|  6|   0|             3|  63.28|       0.912|       0.166| 0.547|         2.32E-4|   0.933|  14.973|     0.0799|  0.234|\n",
      "|007dnwwA3q2fhQ4AC...|         0| 10|   1|             4|139.869|      0.0181|       0.746|  0.82|             0.0|   0.418|   1.596|     0.0503|  0.271|\n",
      "|007qEKeXFHJ2r7rMU...|        25|  6|   0|             4|103.515|      0.0743|       0.707| 0.523|             0.0|   0.252|   5.863|      0.228|  0.605|\n",
      "|00841Yho1fVUDDfA9...|         0| 11|   0|             5|180.478|       0.534|       0.613|0.0981|             0.0|   0.226|  27.133|      0.955|  0.609|\n",
      "|0099y8U9Px3fIJX5L...|         0|  3|   1|             4| 91.476|       0.772|       0.541| 0.462|             0.0|  0.0683|   6.676|     0.0249|  0.461|\n",
      "|009BdcwF2lB7RZiTS...|        11| 10|   1|             4| 78.336|       0.915|       0.219|   0.2|           0.321|   0.284|   11.52|     0.0324|  0.192|\n",
      "|009ltBhNaq5XDC4f3...|        10|  9|   0|             3|157.619|       0.931|       0.452| 0.139|             0.0|  0.0851|  15.348|     0.0407|  0.336|\n",
      "|00AYIzevROJfCstNK...|        17|  8|   0|             4| 90.218|       0.105|       0.531| 0.909|             0.0|  0.0858|   3.525|       0.29|  0.644|\n",
      "|00B1efLzCB1lUNPT8...|         3|  0|   1|             4|171.374|       0.849|       0.485| 0.362|         4.13E-6|  0.0986|   7.889|     0.0889|  0.284|\n",
      "|00BhvVsOJkBMJVyoj...|        21|  7|   0|             4| 81.911|       0.709|       0.624| 0.654|          3.6E-5|  0.0974|   4.538|     0.0545|  0.329|\n",
      "|00CNYGlhmuXQcJU81...|         2|  2|   1|             1|124.193|      0.0473|       0.505| 0.753|         0.00156|   0.181|   9.898|     0.0348|  0.535|\n",
      "|00CYT9MwnEJva7cCy...|         9|  1|   1|             4|105.079|       0.784|       0.867|  0.25|             0.0|   0.132|   7.028|     0.0358|  0.442|\n",
      "|00CZIZmqtInPEWdxG...|        13|  4|   1|             5|  83.73|       0.958|       0.372| 0.246|             0.0|  0.0741|  10.112|     0.0582|  0.257|\n",
      "+--------------------+----------+---+----+--------------+-------+------------+------------+------+----------------+--------+--------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import abs\n",
    "\n",
    "df = df \\\n",
    "    .withColumn(\"loudness\", abs(\"loudness\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2833:==================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---+----+--------------+-------+------------+------------+------+----------------+--------+--------+-----------+-------+--------------------+\n",
      "|                  id|popularity|key|mode|time_signature|  tempo|acousticness|danceability|energy|instrumentalness|liveness|loudness|speechiness|valence|            features|\n",
      "+--------------------+----------+---+----+--------------+-------+------------+------------+------+----------------+--------+--------+-----------+-------+--------------------+\n",
      "|000O8nXpAK5QAppKv...|         5|  1|   1|             4|128.018|     5.12E-4|       0.504|  0.93|          0.0107|   0.281|    2.89|     0.0539|  0.355|[5.0,1.0,1.0,4.0,...|\n",
      "|000kSCs9tKtH1VXI3...|        11|  2|   1|             4| 129.85|       0.863|       0.425| 0.266|             0.0|  0.0989|   6.791|     0.0337|  0.253|[11.0,2.0,1.0,4.0...|\n",
      "|0010mZpCCwlPwoBiB...|        38|  3|   1|             3|124.993|       0.108|       0.527| 0.793|         3.28E-6|   0.144|   4.823|     0.0352|  0.597|[38.0,3.0,1.0,3.0...|\n",
      "|0026hQeV7FZ0PaZpW...|        20| 11|   1|             4|169.358|        0.33|        0.48| 0.737|             0.0|   0.449|   7.023|      0.216|  0.201|[20.0,11.0,1.0,4....|\n",
      "|003Zj5utQfbYDofOq...|        19|  1|   1|             4| 89.971|      0.0117|       0.706| 0.478|          0.0211|   0.116|   7.839|     0.0434| 0.0869|[19.0,1.0,1.0,4.0...|\n",
      "|003heort6ZPgg46RM...|         2|  8|   1|             4| 81.486|       0.886|        0.72| 0.231|           0.806|   0.118|   14.43|     0.0364|  0.742|[2.0,8.0,1.0,4.0,...|\n",
      "|004zTdXYH400nJeJH...|         3|  8|   0|             5|168.081|       0.956|       0.254| 0.133|           0.506|  0.0559|  27.071|     0.0831| 0.0702|[3.0,8.0,0.0,5.0,...|\n",
      "|006x4IC6Fm1xjYtns...|         0|  6|   0|             3|  63.28|       0.912|       0.166| 0.547|         2.32E-4|   0.933|  14.973|     0.0799|  0.234|[0.0,6.0,0.0,3.0,...|\n",
      "|007dnwwA3q2fhQ4AC...|         0| 10|   1|             4|139.869|      0.0181|       0.746|  0.82|             0.0|   0.418|   1.596|     0.0503|  0.271|[0.0,10.0,1.0,4.0...|\n",
      "|007qEKeXFHJ2r7rMU...|        25|  6|   0|             4|103.515|      0.0743|       0.707| 0.523|             0.0|   0.252|   5.863|      0.228|  0.605|[25.0,6.0,0.0,4.0...|\n",
      "|00841Yho1fVUDDfA9...|         0| 11|   0|             5|180.478|       0.534|       0.613|0.0981|             0.0|   0.226|  27.133|      0.955|  0.609|[0.0,11.0,0.0,5.0...|\n",
      "|0099y8U9Px3fIJX5L...|         0|  3|   1|             4| 91.476|       0.772|       0.541| 0.462|             0.0|  0.0683|   6.676|     0.0249|  0.461|[0.0,3.0,1.0,4.0,...|\n",
      "|009BdcwF2lB7RZiTS...|        11| 10|   1|             4| 78.336|       0.915|       0.219|   0.2|           0.321|   0.284|   11.52|     0.0324|  0.192|[11.0,10.0,1.0,4....|\n",
      "|009ltBhNaq5XDC4f3...|        10|  9|   0|             3|157.619|       0.931|       0.452| 0.139|             0.0|  0.0851|  15.348|     0.0407|  0.336|[10.0,9.0,0.0,3.0...|\n",
      "|00AYIzevROJfCstNK...|        17|  8|   0|             4| 90.218|       0.105|       0.531| 0.909|             0.0|  0.0858|   3.525|       0.29|  0.644|[17.0,8.0,0.0,4.0...|\n",
      "|00B1efLzCB1lUNPT8...|         3|  0|   1|             4|171.374|       0.849|       0.485| 0.362|         4.13E-6|  0.0986|   7.889|     0.0889|  0.284|[3.0,0.0,1.0,4.0,...|\n",
      "|00BhvVsOJkBMJVyoj...|        21|  7|   0|             4| 81.911|       0.709|       0.624| 0.654|          3.6E-5|  0.0974|   4.538|     0.0545|  0.329|[21.0,7.0,0.0,4.0...|\n",
      "|00CNYGlhmuXQcJU81...|         2|  2|   1|             1|124.193|      0.0473|       0.505| 0.753|         0.00156|   0.181|   9.898|     0.0348|  0.535|[2.0,2.0,1.0,1.0,...|\n",
      "|00CYT9MwnEJva7cCy...|         9|  1|   1|             4|105.079|       0.784|       0.867|  0.25|             0.0|   0.132|   7.028|     0.0358|  0.442|[9.0,1.0,1.0,4.0,...|\n",
      "|00CZIZmqtInPEWdxG...|        13|  4|   1|             5|  83.73|       0.958|       0.372| 0.246|             0.0|  0.0741|  10.112|     0.0582|  0.257|[13.0,4.0,1.0,5.0...|\n",
      "+--------------------+----------+---+----+--------------+-------+------------+------------+------+----------------+--------+--------+-----------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Create Features\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# selected_features = [\"acousticness\", \"danceability\", \"energy\", \"instrumentalness\", \"liveness\", \"speechiness\", \"valence\"]\n",
    "selected_features = [\"popularity\", \"key\", \"mode\", \"time_signature\", \"tempo\", \"acousticness\", \"danceability\", \"energy\", \"instrumentalness\", \"liveness\", \"loudness\", \"speechiness\", \"valence\"]\n",
    "assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "df_assembled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers:\n",
      "[ 5.30442388e+00  5.25986151e+00  6.33216098e-01  3.90068808e+00\n",
      "  1.26251960e+02  3.85885709e-01  5.81514163e-01  5.61546456e-01\n",
      "  2.81342948e-01  2.00185878e-01 -1.07680766e+01  7.88508693e-02\n",
      "  4.65834796e-01]\n",
      "[ 1.23390427e+01  5.23408698e+00  6.55525586e-01  3.84587689e+00\n",
      "  1.67466397e+02  3.35039040e-01  4.92082926e-01  6.10620140e-01\n",
      "  1.94414210e-01  2.08495522e-01 -9.22966755e+00  1.16590383e-01\n",
      "  4.89392808e-01]\n",
      "[ 3.81960187e+01  5.27521194e+00  6.05877286e-01  3.93880933e+00\n",
      "  1.14890647e+02  2.95583223e-01  6.30137124e-01  6.18648537e-01\n",
      "  1.27659892e-01  1.97130981e-01 -8.14928326e+00  9.33018380e-02\n",
      "  4.76946989e-01]\n",
      "[  6.34248749   5.21078692   0.64991465   3.80168955  86.14469493\n",
      "   0.57447802   0.4768827    0.41216808   0.28717292   0.19524888\n",
      " -13.72237139   0.09805415   0.39089009]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|[5.0,1.0,1.0,4.0,...|         0|\n",
      "|[11.0,2.0,1.0,4.0...|         0|\n",
      "|[38.0,3.0,1.0,3.0...|         2|\n",
      "|[20.0,11.0,1.0,4....|         1|\n",
      "|[19.0,1.0,1.0,4.0...|         3|\n",
      "|[2.0,8.0,1.0,4.0,...|         3|\n",
      "|[3.0,8.0,0.0,5.0,...|         1|\n",
      "|[0.0,6.0,0.0,3.0,...|         3|\n",
      "|[0.0,10.0,1.0,4.0...|         0|\n",
      "|[25.0,6.0,0.0,4.0...|         2|\n",
      "|[0.0,11.0,0.0,5.0...|         1|\n",
      "|[0.0,3.0,1.0,4.0,...|         3|\n",
      "|[11.0,10.0,1.0,4....|         3|\n",
      "|[10.0,9.0,0.0,3.0...|         1|\n",
      "|[17.0,8.0,0.0,4.0...|         3|\n",
      "|[3.0,0.0,1.0,4.0,...|         1|\n",
      "|[21.0,7.0,0.0,4.0...|         3|\n",
      "|[2.0,2.0,1.0,1.0,...|         0|\n",
      "|[9.0,1.0,1.0,4.0,...|         3|\n",
      "|[13.0,4.0,1.0,5.0...|         3|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1846:================================================>     (33 + 4) / 37]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|prediction|prediction|\n",
      "+----------+----------+\n",
      "|         1|         1|\n",
      "|         3|         3|\n",
      "|         2|         2|\n",
      "|         0|         0|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# <----------------------DEMO--------------------------------\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "assembler_demo = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "df_assembled_demo = assembler_demo.transform(df)\n",
    "\n",
    "# create model == train dataset\n",
    "kmeans = KMeans().setK(4).setSeed(1)\n",
    "model = kmeans.fit(df_assembled_demo)\n",
    "\n",
    "# check centers\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers:\")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "\n",
    "# test dataset\n",
    "df_result = model.transform(df_assembled_demo)\n",
    "\n",
    "# check prediction results\n",
    "df_result.select(\"features\", \"prediction\").show()\n",
    "\n",
    "# check mean values\n",
    "df_result.groupBy(\"prediction\").agg(\n",
    "    col(\"prediction\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# vector 컬럼이 1개 이상 존재할 수 없음\n",
    "minmax_scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "minmax_model = minmax_scaler.fit(df_assembled)\n",
    "minmax_scaled_df = minmax_model.transform(df_assembled) \\\n",
    "    # .drop('features') \\\n",
    "    # .withColumnRenamed('scaledFeatures', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2880:==================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+--------------+---------------+-------------------------+----------------+-----------------------+-----------------------+-----------------+---------------------------+-------------------+-------------------+----------------------+------------------+\n",
      "|id_null_count|popularity_null_count|key_null_count|mode_null_count|time_signature_null_count|tempo_null_count|acousticness_null_count|danceability_null_count|energy_null_count|instrumentalness_null_count|liveness_null_count|loudness_null_count|speechiness_null_count|valence_null_count|\n",
      "+-------------+---------------------+--------------+---------------+-------------------------+----------------+-----------------------+-----------------------+-----------------+---------------------------+-------------------+-------------------+----------------------+------------------+\n",
      "|            0|                    0|             0|              0|                        0|               0|                      0|                      0|                0|                          0|                  0|                  0|                     0|                 0|\n",
      "+-------------+---------------------+--------------+---------------+-------------------------+----------------+-----------------------+-----------------------+-----------------+---------------------------+-------------------+-------------------+----------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "minmax_scaled_df \\\n",
    "    .select([sum(col(column).isNull().cast(\"int\")).alias(column + \"_null_count\") for column in df.columns]) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "minmax_scaled_train = minmax_scaled_df.filter(col(\"id\").isin(track_list))\n",
    "minmax_scaled_test = minmax_scaled_df.filter(~col(\"id\").isin(track_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2928:==================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1581448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(len(track_list))\n",
    "print(minmax_scaled_train.count())\n",
    "print(minmax_scaled_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# create model == train dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m kmeans \u001b[39m=\u001b[39m KMeans(featuresCol\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mscaledFeatures\u001b[39;49m\u001b[39m\"\u001b[39;49m, k\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, seed\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m model \u001b[39m=\u001b[39m kmeans\u001b[39m.\u001b[39mfit(minmax_scaled_train)\n\u001b[1;32m      5\u001b[0m \u001b[39m# check centers\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMethod \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m forces keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_kwargs \u001b[39m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/clustering.py:805\u001b[0m, in \u001b[0;36mKMeans.__init__\u001b[0;34m(self, featuresCol, predictionCol, k, initMode, initSteps, tol, maxIter, seed, distanceMeasure, weightCol, solver, maxBlockSizeInMB)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[39m__init__(self, \\\\*, featuresCol=\"features\", predictionCol=\"prediction\", k=2, \\\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[39m         initMode=\"k-means||\", initSteps=2, tol=1e-4, maxIter=20, seed=None, \\\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39m         distanceMeasure=\"euclidean\", weightCol=None, solver=\"auto\", \\\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m         maxBlockSizeInMB=0.0)\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[39msuper\u001b[39m(KMeans, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 805\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_java_obj(\u001b[39m\"\u001b[39;49m\u001b[39morg.apache.spark.ml.clustering.KMeans\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muid)\n\u001b[1;32m    806\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_kwargs\n\u001b[1;32m    807\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msetParams(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:80\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39mReturns a new Java object.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 80\u001b[0m \u001b[39massert\u001b[39;00m sc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     82\u001b[0m java_obj \u001b[39m=\u001b[39m _jvm()\n\u001b[1;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m java_class\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create model == train dataset\n",
    "kmeans = KMeans(featuresCol=\"scaledFeatures\", k=4, seed=1)\n",
    "model = kmeans.fit(minmax_scaled_train)\n",
    "\n",
    "# check centers\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers:\")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "\n",
    "# test dataset\n",
    "df_result = model.transform(minmax_scaled_train)\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
